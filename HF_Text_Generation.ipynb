{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Inference"
      ],
      "metadata": {
        "id": "bWMahIVXQx_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries for transformers (Hugging Face library for NLP)\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "YRZMqNQSRgtu",
        "collapsed": true,
        "outputId": "aa8f6afb-a2b5-49f0-85d8-6c731c1f9b5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Specify the model to use (pretrained from Hugging Face)\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\" # You can use other models or your own fine-tuned one model here\n",
        "task = \"text-generation\"\n",
        "prompt = \"Explain how neural networks work in simple terms\"\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "generator = pipeline(task, model=model_name)\n",
        "\n",
        "# Generate text based on the given prompt\n",
        "results = generator(prompt, max_length=100, truncation=True, num_return_sequences=1)\n",
        "\n",
        "# Print the generated output\n",
        "text = results[0].get(\"generated_text\")  # Extract generated text\n",
        "print(f\"Generated text:\\n{text}\")"
      ],
      "metadata": {
        "id": "R1iCaTp9MYyb",
        "collapsed": true,
        "outputId": "71d7c3a2-d8af-43fd-95c6-c6bc29111785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated text:\n",
            "Explain how neural networks work in simple terms.\n",
            "I know that neural networks are a big part of AI, but I don't really understand how they work. I want to learn more, but I don't know where to start.\n",
            "Okay, so I need to explain neural networks in simple terms. I know a bit about AI, but not much about neural networks specifically. Let me start by recalling what I know about AI.\n",
            "\n",
            "AI is about machines learning and doing tasks that usually require human intelligence\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Fine-Tuning"
      ],
      "metadata": {
        "id": "uJ5aSbkBQ4OD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries for transformers, dataset handling, and logging\n",
        "!pip install -q transformers datasets wandb"
      ],
      "metadata": {
        "id": "fiQCdNtLng5N",
        "collapsed": true,
        "outputId": "21f3a439-fe11-45b2-f603-4eb59ba69e52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/485.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/143.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "import wandb\n",
        "\n",
        "# Log in to Weights & Biases (W&B) using Colab secrets\n",
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "\n",
        "# Define the model for fine-tuning\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "\n",
        "# Create a small custom dataset for training and validation\n",
        "train_data = {\n",
        "    \"text\": [\n",
        "        \"Artificial Intelligence is transforming industries by automating tasks and enabling smarter decision-making.\",\n",
        "        \"Machine learning algorithms learn patterns from data to make predictions and improve over time.\",\n",
        "        \"Neural networks, inspired by the human brain, are the backbone of modern deep learning systems.\",\n",
        "        \"Self-driving cars rely on AI to navigate roads, detect obstacles, and make real-time decisions.\",\n",
        "    ]\n",
        "}\n",
        "\n",
        "val_data = {\n",
        "    \"text\": [\n",
        "        \"AI has the potential to solve some of the world's most pressing challenges, from healthcare to climate change.\",\n",
        "        \"The collaboration between humans and AI will define the next era of technological innovation.\"\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "kZQtIwDfllT_",
        "outputId": "5f74bbca-301c-4a81-b885-b59cb864c910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SecretNotFoundError",
          "evalue": "Secret WANDB_API_KEY does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSecretNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-a88f52b6d707>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Log in to Weights & Biases (W&B) using Colab secrets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muserdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'WANDB_API_KEY'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Define the model for fine-tuning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/userdata.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(key)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'exists'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mSecretNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'access'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mNotebookAccessError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSecretNotFoundError\u001b[0m: Secret WANDB_API_KEY does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "# For models like GPT-2 that don't have a pad token, assign the eos token as pad_token.\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Convert dataset into Hugging Face Dataset format\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n",
        "\n",
        "# Define tokenization function with truncation and padding\n",
        "max_length = 64  # Maximum sequence length\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=max_length)\n",
        "\n",
        "# Apply tokenization to datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Add labels by copying input_ids\n",
        "# The labels are simply the input_ids because the model learns to predict the next token.\n",
        "tokenized_train = tokenized_train.map(lambda examples: {\"labels\": examples[\"input_ids\"]})\n",
        "tokenized_val = tokenized_val.map(lambda examples: {\"labels\": examples[\"input_ids\"]})"
      ],
      "metadata": {
        "id": "dgxzv_JVl5me",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"AI-Tuned-DeepSeek-R1-Distill-Qwen-1.5B\",  # Directory to save model checkpoints\n",
        "    learning_rate=0.0001,  # Learning rate for fine-tuning\n",
        "    per_device_train_batch_size=2,  # Reduce batch size for low memory usage\n",
        "    per_device_eval_batch_size=2,\n",
        "    eval_strategy=\"steps\",  # Evaluate periodically during training\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    eval_steps=2,  # Perform evaluation every 2 steps\n",
        "    logging_steps=2,  # Log training details every 2 steps\n",
        "    save_steps=2,  # Save model checkpoints every 2 steps\n",
        "    load_best_model_at_end=True,  # Load best model after training\n",
        "    report_to=[\"wandb\"],  # Log metrics to Weights & Biases\n",
        "    push_to_hub=True,  # Upload model to Hugging Face Hub\n",
        ")\n",
        "\n",
        "# Initialize the Trainer class\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "\n",
        "# Fine-tune the model (this will log training metrics to W&B and evaluate on the validation set)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "44-ykrRBRQ1N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}